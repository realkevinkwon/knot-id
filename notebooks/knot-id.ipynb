{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIAOzEUn_jZv"
      },
      "source": [
        "# Knot ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpCTmufo_jZw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOa4Cdm6_jZx"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsWp2-pr_jZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206c2ac6-6cae-4abe-948f-396ada4fdffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL8cSifFC3GL"
      },
      "source": [
        "### Google Drive setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNnot8p1C5ir",
        "outputId": "621c6829-a356-4a4f-b4ee-66774e78215e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7uNbWsVBQ18"
      },
      "source": [
        "### GPU setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ld-nzZDsBVWP"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nur3Ewr8_jZy"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6ekIMln8_jZy"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "img_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-4\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/datasets/knot-id'\n",
        "DATA_ROOT_PROCESSED = os.path.join(PROJECT_ROOT, 'data/processed')\n",
        "DATA_DIR = f'10Knots_{img_size}'\n",
        "DATA_PATH = os.path.join(DATA_ROOT_PROCESSED, DATA_DIR)\n",
        "CLASSES = [\n",
        "\t'Alpine Butterfly Knot',\n",
        "\t'Bowline Knot',\n",
        "\t'Clove Hitch',\n",
        "\t'Figure-8 Knot',\n",
        "\t'Figure-8 Loop',\n",
        "\t'Fisherman\\'s Knot',\n",
        "\t'Flemish Bend',\n",
        "\t'Overhand Knot',\n",
        "\t'Reef Knot',\n",
        "\t'Slip Knot'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBZXv2oQ_jZy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvqp-Hmj_jZy"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jtoPVu4a_jZy"
      },
      "outputs": [],
      "source": [
        "class Knots(VisionDataset):\n",
        "\n",
        "\tdef __init__(self, split, transform=None):\n",
        "\t\tself.transform = transform\n",
        "\t\tself.filepaths = []\n",
        "\t\tself.targets = []\n",
        "\t\tself.split = split\t\n",
        "\t\tsuper().__init__(DATA_ROOT_PROCESSED, transforms=None, transform=transform)\n",
        "\n",
        "\t\tfor idx, class_name in enumerate(CLASSES):\n",
        "\t\t\tclass_path = os.path.join(DATA_PATH, self.split, class_name)\n",
        "\t\t\tfor file in os.listdir(class_path):\n",
        "\t\t\t\tif file != '.DS_Store':\n",
        "\t\t\t\t\tself.filepaths.append(os.path.join(class_path, file))\n",
        "\t\t\t\t\tself.targets.append(idx)\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.filepaths)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\timg = Image.open(self.filepaths[idx])\n",
        "\t\ttarget = self.targets[idx]\t\n",
        "\t\tif self.transform is not None:\n",
        "\t\t\timg = self.transform(img)\t\n",
        "\t\treturn img, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "jDP2AZP1g9hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_plot(model_id, train_losses, test_losses, train_accuracies, test_accuracies, epochs):\n",
        "\tfig, (ax1, ax2) = plt.subplots(2,1)\n",
        "\tfig.suptitle('Loss and Classification Accuracy', x=0.13, y=0.95, ha='left')\n",
        "\t\n",
        "\tax1.plot(epochs, train_losses, 'o-', label='train')\n",
        "\tax1.plot(epochs, test_losses, '.-', label='test')\n",
        "\tax1.set_ylabel('Loss')\n",
        "\tax1.legend(\n",
        "\t\tbbox_to_anchor=(0.66, 1.15, 0.35, 0.102),\n",
        "\t\tloc='upper right',\n",
        "\t\tncol=2,\n",
        "\t\tmode='expand'\n",
        "\t)\n",
        "\n",
        "\tax2.plot(epochs, train_accuracies, '.-', label='train')\n",
        "\tax2.plot(epochs, test_accuracies, '.-', label='test')\n",
        "\tax2.set_ylabel('Classification accuracy (%)')\n",
        "\tax2.set_xlabel('Epochs')\n",
        "\n",
        "\tplt.savefig(f'./models/figures/knot-id_{model_id:04}.png')"
      ],
      "metadata": {
        "id": "skRKhZOGhHI_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLm8190J_jZz"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OJn_OcCr_jZz"
      },
      "outputs": [],
      "source": [
        "class KnotID(nn.Module):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(KnotID, self).__init__()\n",
        "\n",
        "\t\t# Layers for learning features\n",
        "\t\t# input shape (1, 3, 128, 128)\n",
        "\t\tself.feature_learning = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(3, 10, 3, 1, 1),      # (1, 10, 128, 128)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2),                # (1, 10, 64, 64)\n",
        "\t\t\tnn.Conv2d(10, 20, 3, 1, 1),     # (1, 20, 64, 64)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2),\t\t\t\t# (1, 20, 32, 32)\n",
        "\t\t\tnn.Conv2d(20, 40, 3, 1, 1),     # (1, 40, 32, 32)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2),\t\t\t\t# (1, 40, 16, 16)\n",
        "\t\t\tnn.Conv2d(40, 80, 3, 1, 1),     # (1, 80, 16, 16)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2),\t\t\t\t# (1, 80, 8, 8)\n",
        "\t\t\tnn.Conv2d(80, 160, 3, 1, 1),    # (1, 160, 8, 8)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2),\t\t\t\t# (1, 160, 4, 4)\n",
        "\t\t)\n",
        "\n",
        "\t\t# Layers for classifying images\n",
        "\t\tself.classification = nn.Sequential(\n",
        "\t\t\tnn.Flatten(1),                  # (1, 2560)\n",
        "\t\t\tnn.Linear(2560, 768),        \t# (1, 768)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(768, 256),        \t# (1, 256)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(256, 64),        \t\t# (1, 64)\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(64, 10),        \t\t# (1, 10)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.feature_learning(x)\n",
        "\t\tx = self.classification(x)\n",
        "\t\treturn x\n",
        "\n",
        "\tdef save(self, model_id, data, train_time):\n",
        "\t\tdf = pd.DataFrame(data)\n",
        "\t\tdf.to_csv(os.path.join(PROJECT_ROOT, f'/models/tables/knot-id_{model_id:04}.csv'))\n",
        "\n",
        "\t\tsave_plot(\n",
        "\t\t\tmodel_id,\n",
        "\t\t\tdata['train_losses'],\n",
        "\t\t\tdata['test_losses'],\n",
        "\t\t\tdata['train_accuracies'],\n",
        "\t\t\tdata['test_accuracies'],\n",
        "\t\t\tdata['epochs']\n",
        "\t\t)\n",
        "\n",
        "\t\tmodel_summary = str(summary(self, input_size=(1,3,128,128), verbose=0))\n",
        "\n",
        "\t\twith open(os.path.join(PROJECT_ROOT, f'/models/summaries/knot-id_{model_id:04}.txt'), 'w') as file:\n",
        "\t\t\tfile.write(f\"img_size: {data['img_size']}\\n\")\n",
        "\t\t\tfile.write(f\"batch_size: {data['batch_size']}\\n\")\n",
        "\t\t\tfile.write(f\"learning_rate: {data['learning_rate']}\\n\")\n",
        "\t\t\tfile.write(f\"num_epochs: {data['num_epochs']}\\n\\n\")\n",
        "\t\t\tfile.write(f'training time: {train_time:}\\n\\n')\n",
        "\t\t\tfile.write(f'{str(self)}\\n\\n')\n",
        "\t\t\tfile.write(model_summary)\n",
        "\n",
        "\t\ttorch.save(self.state_dict(), os.path.join(PROJECT_ROOT, f'/models/serialized/knot-id_{model_id:04}.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1rTJ9t7_jZ0"
      },
      "source": [
        "## Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s1lfMnLb_jZ0"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, loss_fn, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = loss_fn(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}]'\n",
        "                f'Loss: {loss.item():.8f}'\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TNm-LYBh_jZ0"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, loss_fn, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_accuracy = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            output = model(images)\n",
        "\n",
        "            test_loss += loss_fn(output, targets).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "        print(\n",
        "            f'Test result on epoch {epoch}: '\n",
        "            f'Avg loss is {test_loss:.4f}, '\n",
        "            f'Accuracy: {test_accuracy:.2f}%'\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_AoGv99_jZ0"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize((0.7019, 0.4425, 0.1954), (0.1720, 0.1403, 0.1065))\n",
        "])\n",
        "\n",
        "train_data = Knots(split='train', transform=transform)\n",
        "test_data = Knots(split='test', transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "8n10szYNheol"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH3Rb1TF_jZ1"
      },
      "outputs": [],
      "source": [
        "model_id = 42\n",
        "\n",
        "model = KnotID()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "epochs = range(1, num_epochs+1)\n",
        "\n",
        "train_start = time.time()\n",
        "for epoch in epochs:\n",
        "    train(model, train_loader, loss_fn, optimizer, epoch)\n",
        "\n",
        "    print('Testing model on train dataset')\n",
        "    loss, accuracy = test(model, train_loader, loss_fn, epoch)\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(accuracy)\n",
        "    print('Testing model on test dataset')\n",
        "    loss, accuracy = test(model, test_loader, loss_fn, epoch)\n",
        "    test_losses.append(loss)\n",
        "    test_accuracies.append(accuracy)\n",
        "train_time = time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start))\n",
        "\n",
        "data = {\n",
        "    'batch_size': batch_size,\n",
        "    'img_size': img_size,\n",
        "    'num_epochs': num_epochs,\n",
        "    'learning_rate': learning_rate,\n",
        "    'epochs': epochs,\n",
        "    'train_losses': train_losses,\n",
        "    'test_losses': test_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'test_accuracies': test_accuracies\n",
        "}\n",
        "model.save(model_id, data, train_time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "03bb1b95e98afcf6a2640b6976662dfa86aafc795349b15c3689776271668947"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}